<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Explainer Page</title>
    <link rel="stylesheet" href="lib/bootstrap-4.0.0-dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="styles.css">
    <link rel="icon" href="favicon.ico" type="image/x-icon"/>
</head>
<body>
<div class="container mt-3">
    <h1 class="text-center">Melbourne's Pedestrian Sensor Network Explainer Page</h1>
    <h2>Motivation</h2>
    <p>
        While looking for potential datasets for this project, the search was heavily influenced by the play animation
        of Assignment 2, that animated the locations of murders in NYC over time.
        This directed the focus, during the search for interesting datasets, towards those containing geo locations and
        timestamps, in order to create a similar style of animation.
        The central dataset, that was settled on, is based on a network of pedestrian sensors in the city on Melbourne.
        The dataset contains the amount of hourly registrations for each sensor and was chosen because it provides a
        solid foundation for an interesting map based visualization, the idea of exploring the driving forces behind
        pedestrian movement in a major city seemed interesting and multiple other datasets and statistics about
        Melbourne's population, weather and employment exist, that could be used to supplement the main dataset in order
        to create an narrative.
        The focus for the user’s experience was to provide a highly interactive visualisation that provides an overview
        of the daily pedestrian movement as well each sensors registrations.
    </p>
    <h2>Basic Stats</h2>
    <p>
        The main dataset contains about 2.21 million rows and is with a filesize of 166 MB far too large to use without
        preprocessing. The raw dataset contains redundant information about the date of each observation, but even after
        removing those attributes, the file remains too large. Storing each observation in a list would also come with
        huge performance problems when trying to find all observations of a specific day and further a specific sensors
        observations for that day.
        To avoid this problem, the observations need to be grouped by day and further by sensor before using the data in
        the browser. However, this would not solve the problem regarding the size of the file.
    </p>
    <p>
        Fortunately a publicly available API provides all observations by day. Using the API, the data can be broken
        down into manageable pieces, but also introduces a new set of problems.
        Firstly, the API does not allow Cross Origin Resource Sharing (<a
            href="https://en.wikipedia.org/wiki/Cross-origin_resource_sharing" target="_blank">CORS</a>) , which means,
        that a <a href="https://en.wikipedia.org/wiki/Proxy_server" target="_blank"> proxy server</a> had to be created
        in order to access the data from a different domain.
    </p>
    <div>
        Secondly, the data about each sensor’s location, name, description and the day it was installed is stored in a
        separate dataset with a unique id used to map between the sensors locations and observations datasets.
    </div>
    <div>
        However, the API does not use the sensor id to map to the sensor locations dataset, but instead the sensor’s
        name. The names are also unique, but not consistent with all names in the locations dataset. In order to be able
        to map all observations provided by the API to a sensor location, a dictionary that maps between sensor ids and
        multiple names had to be created manually.
    </div>
    <div>
        The detection of unmapped sensors has its difficulties which means, that it is not guaranteed that all name mismatches have been found.
    </div>
    <p>
        While the API is used to provide for the visualisation, the dataset with all sensor observations was still used
        for data analysis.
        As mentioned earlier, some other datasets where used apart from the sensor observations and locations.
    </p>
    <p>
        Some statistics about Melbourne's population and employment per suburb where manually compiled from various
        different government sources.
        A geojson file is used to display a map of melbourne’s suburbs. After it became apparent, that the sensor
        network only spans close to the city’s center, the geojson was filtered to only contain the relevant suburbs.
    </p>
    <p>
        An additional two other data sets has been used. The first one contains information on rainfall in Melbourne. It
        consists of 19549 rows with a filesize of 722 KB.
        We wanted to match all days in the pedestrian dataset, to be able to lookup the rainfall amount for any
        particular day. The relevant data was added to a dictionary and sorted by rainfall amount. This allowed us to
        easily lookup dates with heavy rain to further investigate if it had impacted the pedestrian traffic.
    </p>
    <p>
        The second additional dataset consists of temperature measurements from Essendon Airport VIC. it contains 28967
        rows with a filesize of 1.1 MB. As with the rainfall dataset we only wanted the days that were in pedestrian
        dataset (to make comparisons).
        The date and temperature were added to a dictionary which enabled us to calculate the correlation between the
        temperature and number of pedestrians within a certain timeframe.
        Also we were able to create scatterplots to see if any clusters or patterns showed up.
        One example is the scatterplots of the temperature vs number of pedestrians in each season.
        This showed us that most of the high volume of pedestrians were located in the spring.
    </p>
    <div>
        <img src="spring.png" width="100%">
    </div>
    <p>
        We sorted the temperature data into weekdays to further study the correlation of the weather and pedestrian
        traffic in a more specific time frame.
    </p>
    <h2>Genre</h2>
    <p>
        We have created a magazine style as we have interactive visualization that is within a single frame embedded with text.
    </p>
    <p>
        For our visual narrative we have the following elements:
    </p>
    <div>
        <ul>
            <li>
                Visual Structuring
                <ul>
                    <li>Consistent visual platform</li>
                    <li>Timebar</li>
                </ul>
            </li>
            <li>
                Highlighting
                <ul>
                    <li>Feature distinction</li>
                    <li>Motion</li>
                </ul>
            </li>
            <li>
                Transition Guidance
                <ul>
                    <li>Familiar Objects</li>
                    <li>Animated transitions</li>
                </ul>
            </li>
        </ul>
    </div>
    <p>
        We have used a consistent visual platform as it makes the most sense to us when we are using a one-pager where we wants to keep it organized. As the user has to potentially navigate through nine years of data, we saw it as an obvious choice to implement a timebar / calendar to help the user navigate. We use feature distinction to both highlight different areas as well as sensors on the map. Furthermore the user is able to highlight the different graphs by hovering the corresponding legend. All of this is done to make it easier for the user to understand and interact with the visualization, as we don't provide major guidance. Our animated transitions is used to show a distinction when comparing the data.
    </p>
    <p>
        For the narrative Structure we use:
    </p>
    <div>
        <ul>
            <li>
                Ordering
                <ul>
                    <li>User Directed Path</li>
                </ul>
            </li>
            <li>
                Interactivity
                <ul>
                    <li>Hover Highlighting / Details</li>
                    <li>Filtering / Selection / Search</li>
                    <li>Navigation Buttons</li>
                </ul>
            </li>
            <li>
                Messaging
                <ul>
                    <li>Captions / Headlines</li>
                    <li>Accompanying Article</li>
                    <li>Introductory</li>
                    <li>Summary / Synthesis</li>
                </ul>
            </li>
        </ul>
    </div>
    <p>
        Our narrative uses a user directed path as we like to give the user the option to discover patterns and make observations on their own. To help the user navigate we use filtering, selection and search. As mentioned in the earlier section the user can use the calendar which uses these features to easier get to a specific date or read data from a certain sensor. Accompanying articles are included in our narrative to reference our statements but also to give the user a chance to read further into the story. We have added both introductory and summary to make it fast and easy to understand our findings during this project.
    </p>
    <h2>Visualisations</h2>
    <p>
        Two different kinds of visualisations where chosen for this project: a map and a line chart.
        A map of Melbourne is the central visualisation and displays each of the sensors location as well as the sensor’s hourly pedestrian volume as a percentage of the total hourly volume across all sensors. This provides a good overview of the busiest parts of the city especially when playing the animation for the whole day.
    </p>
    <p>
        The map can also be turned into a choropleth, which provides a better understanding the population's distribution across Melbourne’s suburbs.
        The interface also lets the user chose to view a certain day, or to go through the day back and forth one day at a time, giving users an opportunity to dive deep into the data themselves.
        When a sensor is selected on the map the line chart shows the sensors hourly volume across the whole day, as well as each hours average volume for the past 4 and 52 weeks.
        Showing the sensors current counts together with the two averages gives an good indication if the current day is deviating from the long term trends.

    </p>
    <p>
        The two visualisations provide a good overview of the daily pedestrian traffic, but they do not tell much of a story by themselves, which will be further discussed in the next section.
    </p>
    <h2>Discussion</h2>
    <p>
        Overall the final product achieves the level of interactivity, that was envisioned in the beginning of this project and is actually fun to mess around with.
        The visualisation’s overall quality is on a satisfactory level and despite the quite large dataset, it’s possible to explore the full dataset.
    </p>
    <p>
        However, there is a disconnect in the visualisation between the macroscopic and microscopic level. When the datasets were analysed, it was mostly done by comparing the total daily observations to understand the magnitude and scale of the data, but there aren’t any visualisations that communicate those findings to the user.
    </p>
    <p>
        The visualisations can still illustrate some the driving forces behind the pedestrian volume, like population and time, but that is most likely not enough going to catch users and motivate them to dive deep into the dataset themselves, despite interactivity being a big focus.
        Instead using a martini glass approach, it feels more like we spilled the drink all over the customers and hope that they’ll enjoy it.
    </p>
    <p>
        It is also debatable if an magazine style visualisation is the right approach for this project.
        There is a strong case to be made, that an interactive slide show would be a better approach.
        Splitting the visualisation up into slides would help creating a more thorough visualisation, that allows to look at the data from both a large, general angle as well as a small, detailed angle and understand the connection between them while avoiding visual cluttering and information overload.
        In order to implement this though, a lot more data analysis and preparation would need to be done
    </p>
    <p>
        The data analysis is another area that could be improved. There is too much use of absolute values during the analysis. This is problematic because the number of sensors has increased over the years which very likely will cause an increase in observations just by itself.
        Using more sophisticated, statistical methods would probably help to reveal more details about the data and also give more weight to the findings.
    </p>
    <h2>Contributions</h2>
    <div style="min-height: 300px">
        <p>
            <a data-toggle="collapse" href="#collapseExample" role="button" aria-expanded="false" aria-controls="collapseExample">
                Only click if you are Sune, or one of his TA's
            </a>
        </p>
        <div class="collapse" id="collapseExample">
            <div>Yejin Kim's main responsibilities
                <ul>
                    <li>Video</li>
                </ul>
            </div>
            <div>Jacob Bundgaard Knudsen's main responsibilities
                <ul>
                    <li>Data Exploration and Analysis
                    </li>
                    <li>Narrative</li>
                </ul>
            </div>
            <div>Jan-Eric Raab's main responsibilities
                <ul>
                    <li>Visualisation and Website</li>
                    <li>Explainer Page</li>
                </ul>
            </div>
        </div>
    </div>
</div>
<script src="lib/jquery-3.3.1.slim.min.js"></script>
<script src="lib/popper.min.js"></script>
<script src="lib/bootstrap-4.1.0-dist/js/bootstrap.min.js"></script>
</body>
</html>
